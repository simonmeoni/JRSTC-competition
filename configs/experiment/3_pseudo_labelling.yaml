# @package _global_

# to execute this experiment run:
# python run.py experiment=baseline.yaml

defaults:
  - override /mode: default.yaml
  - override /trainer: default.yaml
  - override /model: roberta_model.yaml
  - override /datamodule: datamodule.yaml
  - override /callbacks: wandb.yaml
  - override /logger: wandb.yaml # set logger here or use command line (e.g. `python run.py logger=wandb`)

  # enable color logging
  - override /hydra/job_logging: colorlog
  - override /hydra/hydra_logging: colorlog

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: Fold-${current_fold}
current_fold: 0
seed: 12345
transformer_model: "unitary/unbiased-toxic-roberta"
architecture_model: "concat_regression"
experiment_group: "pseudo-labelling"

trainer:
  max_epochs: 6

model:
  lr: 5e-5
  weight_decay: 1e-6
  scheduler: ReduceLROnPlateau
  architecture: ${architecture_model}
  model: ${transformer_model}
  tokenizer: ${transformer_model}
  loss: margin_loss
  remove_dropout: true
  llrd: true

datamodule:
  data_dir: ${data_dir}/jigsaw-classification-voting-cleaning/validation_data.csv
  val_data_dir: ${data_dir}/jigsaw-toxic-severity-rating/validation_data.csv
  train_batch_size: 24
  val_batch_size: 64
  k_fold: 5

callbacks:
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar

logger:
  wandb:
    project: "jrstc-competition"
    group: ${architecture_model}/${transformer_model}/${experiment_group}

work_dir: ${hydra:runtime.cwd}

# path to folder with data
data_dir: ${work_dir}/data

# pretty print config at the start of the run using Rich library
print_config: True
test_after_training: False
